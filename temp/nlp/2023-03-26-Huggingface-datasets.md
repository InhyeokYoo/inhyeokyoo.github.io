---
title:  "ğŸ¤— Huggingface: datasets"
toc: true
toc_sticky: true
permalink: /project/nlp/Hugging-Face/datasets
categories:
  - NLP
  - Huggingface
  - datasets
tags:
use_math: true
last_modified_at: 2023-04-23
---

TODO-list:
- Datasets í´ë˜ìŠ¤

## `Dataset`


### `select`

temp.select(indices: Iterable, keep_in_memory: bool = False, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, new_fingerprint: Optional[str] = None) -> 'Dataset'
Docstring:
Create a new dataset with rows selected following the list/array of indices.

Args:
    indices (`range`, `list`, `iterable`, `ndarray` or `Series`):
        Range, list or 1D-array of integer indices for indexing.
        If the indices correspond to a contiguous range, the Arrow table is simply sliced.
        However passing a list of indices that are not contiguous creates indices mapping, which is much less efficient,
        but still faster than recreating an Arrow table made of the requested rows.
    keep_in_memory (`bool`, defaults to `False`):
        Keep the indices mapping in memory instead of writing it to a cache file.
    indices_cache_file_name (`str`, *optional*, defaults to `None`):
        Provide the name of a path for the cache file. It is used to store the
        indices mapping instead of the automatically generated cache file name.
    writer_batch_size (`int`, defaults to `1000`):
        Number of rows per write operation for the cache file writer.
        This value is a good trade-off between memory usage during the processing, and processing speed.
        Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.
    new_fingerprint (`str`, *optional*, defaults to `None`):
        The new fingerprint of the dataset after transform.
        If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.

Example:

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.select(range(4))
Dataset({
    features: ['text', 'label'],
    num_rows: 4
```


## `DatasetDict`

ì´ë¦„ì„ í‚¤ê°’(e.g. train, test)ìœ¼ë¡œí•˜ë©° `Dataset` ì˜¤ë¸Œì íŠ¸ë¥¼ ë°¸ë¥˜ë¡œí•˜ëŠ” ë”•ì…”ë„ˆë¦¬ì´ë‹¤.
ì´ì—ëŠ” `map`ì´ë‚˜ `filter`ì™€ ê°™ì€ transform ë©”ì†Œë“œë¥¼ ì§€ì›í•˜ë©°, ëª¨ë“  ë°¸ë¥˜ì— ëŒ€í•´ í•œë²ˆì— ì§„í–‰í•  ìˆ˜ ìˆë‹¤.

## ìŠ¤íŠ¸ë¦¬ë°

JSONL, csv, zip, gzip, zstandard ë“±ìœ¼ë¡œ ì••ì¶•ëœ í…ìŠ¤íŠ¸ ê°™ì´ í•œ ì¤„ì”© ì½ëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì••ì¶•/ë¹„ì••ì¶• íŒŒì¼ í¬ë§·ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.
ì•„ë˜ì™€ ê°™ì´ `load_dataset`ì—ì„œ `streaming=True`ë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤.

```py
import datasets

streamed_dataset:  datasets.IterableDataset = datasets.load_dataset('./codeparrot', split="train", streaming=True)
```

í•˜ë“œì— ìºì‹œ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë§ì€ ì–‘ì˜ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.
ìƒˆë¡œìš´ ë°°ì¹˜ê°€ í•„ìš”í•  ë•Œ raw íŒŒì¼ì„ ë°”ë¡œ ì½ì–´ í•´ë‹¹ ë°°ì¹˜ë§Œ ë©”ëª¨ë¦¬ì— ë¡œë“œí•œë‹¤.
ë˜í•œ, ë¡œì»¬ ë¿ë§Œì´ ì•„ë‹ˆë¼ í—ˆë¸Œì— ìˆëŠ” ë°ì´í„°ì…‹ì„ ì§€ì •í•  ìˆ˜ë„ ìˆë‹¤.
ì´ ê²½ìš°ì—” ë¡œì»¬ì— raw íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í•˜ì§€ ì•Šê³  ìƒ˜í”Œì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•œë‹¤.



{: .align-center}{: width="300"}
