---
title:  "[ìž‘ì„± ì¤‘] Big Bird: Transformers for Longer Sequences review"
toc: true
toc_sticky: true
permalink: /project/nlp/Big-Bird/
categories:
  - NLP
  - Paper Review
tags:
  - Language Modeling
use_math: true
last_modified_at: 2022-09-04
---

## Introduction

[A Survey of Transformers](https://arxiv.org/abs/2106.04554)

## Challenging

the full self-attention have computational and memory requirement that is quadratic in the sequence length. 
while the corpus can be large, the sequence length, which provides the context in many applications
is very limited.

![image](https://user-images.githubusercontent.com/47516855/188264775-959f7289-419a-4f3f-ba2a-586062eaeb7e.png){: .align-center}{: width="500"}



> Self-attention plays an important role in Transformer, but there are two challenges in practical applications.
> 1. Complexity. As discussion in Sec. 2.3, the complexity of self-attention is O (ð‘‡ 2 Â· ð·). Therefore, the attention module becomes a bottleneck when dealing with long sequences.
> 2. Structural prior. Self-attention does no assume any structural bias over inputs. Even the order information is also needed to be learned from training data. Therefore, Transformer (w/o pre-training) is usually easy to overfit on small or moderate-size data.
> 
> The improvements on attention mechanism can be divided into several directions:
> 1. Sparse Attention. This line of work introduces sparsity bias into the attention mechanism, leading to reduced complexity.
> 2. Linearized Attention. This line of work disentangles the attention matrix with kernel feature maps. The attention is then computed in reversed order to achieve linear complexity.
> 3. Prototype and Memory Compression. This class of methods reduces the number of queries or key-value memory pairs to reduce the size of the attention matrix.
> 4.  Low-rank Self-Attention. This line of work capture the low-rank property of self-attention.
> 5.  Attention with Prior. The line of research explores supplementing or substituting standard attention with prior attention distributions.
> 6.  Improved Multi-Head Mechanism. The line of studies explores different alternative multi-head mechanisms.

BigBirdì˜ ê²½ìš°ì—” Sparse


However, while we know that self-attention and Transformers are useful, our theoretical understanding
is rudimentary. What aspects of the self-attention model are necessary for its performance? What
can we say about the expressivity of Transformers and similar models?

For example, the
self-attention does not even obey sequence order as it is permutation equivariant

permutation equivariant

Permutation equivarianceëŠ” adjacency matrixì˜ ìˆœì„œê°€ ë°”ë€ŒëŠ”ëŒ€ë¡œ outputì˜ ìˆœì„œë„ ë°”ë€ŒëŠ” í•¨ìˆ˜ì˜ íŠ¹ì„±ì„ ë§í•œë‹¤. ì´ë¥¼ ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

a model that does not produces the same output regardless of the order of elements in the input vector.

https://www.quora.com/What-does-it-mean-that-a-neural-network-is-invariant-to-permutation-When-does-this-happen

This concern has
been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture
all continuous sequence to sequence functions with a compact domain. Meanwhile, PÃ©rez et al. [72]
showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two
natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention
scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity
and flexibility of the original network?

Two natural questions arise: 
- Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products?
- Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?

## Contributions

In this paper, we address both the above questions and produce a sparse attention mechanism that
improves performance on a multitude of tasks that require long contexts. We systematically develop
BIGBIRD, an attention mechanism whose complexity is linear in the number of tokens (Sec. 2). We
take inspiration from *graph sparsification* methods and understand where the proof for expressiveness
of Transformers breaks down when full-attention is relaxed to form the proposed attention pattern.
This understanding helped us develop BIGBIRD, which is theoretically as expressive and also
empirically useful. In particular, our BIGBIRD consists of three main part:

- A set of $g$ global tokens attending on all parts of the sequence.
- All tokens attending to a set of $w$ local neighboring tokens.
- All tokens attending to a set of $r$ random tokens.

This leads to a high performing attention mechanism scaling to much longer sequence lengths (8x).
To summarize, our main contributions are:

1. BIGBIRD satisfies all the known theoretical properties of full transformer (Sec. 3). In particular,
we show that adding extra tokens allows one to express all continuous sequence to sequence
functions with only O(n)-inner products. Furthermore, we show that under standard assumptions
regarding precision, BIGBIRD is Turing complete.
2. Empirically, we show that the extended context modelled by BIGBIRD benefits variety of NLP
tasks. We achieve state of the art results for question answering and document summarization on
a number of different datasets. Summary of these results are presented in Sec. 4.
3. Lastly, we introduce a novel application of attention based models where long contexts are
beneficial: extracting contextual representations of genomics sequences like DNA. With longer
masked LM pretraining, BIGBIRD improves performance on downstream tasks such as promoter-
region and chromatin profile prediction (Sec. 5)

## Related work

There have been a number of interesting attempts, that were aimed at alleviating the quadratic
dependency of Transformers, which can broadly categorized into two directions

First line of work
embraces the length limitation and develops method around it. Simplest methods in this category
just employ sliding window (Multi-passage bert), but in general most work fits in the following general paradigm:
using some other mechanism select a smaller subset of relevant contexts to feed in the transformer
and optionally iterate, i.e. call transformer block multiple time with different contexts each time.
Most prominently, SpanBERT [ 42 ], ORQA [54 ], REALM [ 34], RAG [57] have achieved strong
performance for different tasks. However, it is worth noting that these methods often require significant
engineering efforts (like back prop through large scale nearest neighbor search) and are hard to train.

Second line of work questions if full attention is essential and have tried to come up with approaches
that do not require full attention, thereby reducing the memory and computation requirements.
Prominently, Transformer-xl, [Sukhbaatar et al.](https://aclanthology.org/P19-1032/), compressive transformer have proposed auto-regresive models
that work well for left-to-right language modeling but suffer in tasks which require bidirectional
context. Child et al. [16] proposed a sparse model that reduces the complexity to O(nâˆšn), Kitaev
et al. [49] further reduced the complexity to O(n log(n)) by using LSH to compute nearest neighbors.

Finally,
our work is closely related to and built on the work of Extended Transformers Construction [4].
This work was designed to encode structure in text for transformers. The idea of global tokens was
used extensively by them to achieve their goals. Our theoretical work can be seen as providing
a justification for the success of these models as well. It is important to note that most of the
aforementioned methods are heuristic based and empirically are not as versatile and robust as the
original transformer, i.e. the same architecture do not attain SoTA on multiple standard benchmarks.
(There is one exception of Longformer which we include in all our comparisons, see App. E.3 for a
more detailed comparison). Moreover, these approximations do not come with theoretical guarantees.

E.3 Relationship to Contemporary Work

Longformer introduced localized sliding window to reduce computation. A
more recent version, which includes localized sliding windows and global tokens was introduced
independently by Longofrmer[8]. Although BIGBIRD contains additional random tokens, there are
also differences in the way global and local tokens are realized. In particular even when there is no
random token, as used to get SoTA in question answering, there are two key differences between
Longformer and BIGBIRD-etc (see [4]):
1. We use global-local attention with relative position encodings enables it to better handle
structured inputs
2. Unlike Longformer, we train the global tokens using CPC loss and learn their use during
finetuning.

## Method

### BigBird

*Generalised attention mechanism*ì€ transformer ë ˆì´ì–´ ë‚´ì—ì„œ ì–´ë–¤ input sequence $\mathbf X = (\mathbf x _1, \cdots, \mathbf x _n \in \mathbb R ^{n \times d} $ì— ëŒ€í•´ ë™ìž‘í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì¼ì»«ëŠ”ë‹¤.
Generalized attention mechanismì€ directed graph $D$ë¡œ í‘œí˜„ë˜ë©°, ì´ì˜ vertex set(ê·¸ëž˜í”„ ë‚´ ëª¨ë“  ë…¸ë“œì˜ ì§‘í•©)ì€ $[n] = \{1, \cdots, n \}$ë¡œ í‘œí˜„ëœë‹¤. Arc set(directed edgesì˜ ì§‘í•©)ì€ ì–´í…ì…˜ì´ ì ìš©ë  inner productì˜ ì§‘í•©ìœ¼ë¡œ í‘œí˜„ëœë‹¤.

ì—¬ê¸°ì„œ $N(i)$ë¥¼ ë…¸ë“œ $i$ì˜ *out-neighbors(outgoing neighbors)*ë¼ í•˜ìž. Out-neighborsë¼ í•¨ì€ ì–´ë–¤ ë…¸ë“œ($i$)ì—ì„œ ì¶œë°œí•˜ëŠ” ì—£ì§€ê°€ ìžˆëŠ” ë…¸ë“œë“¤ì„ ì˜ë¯¸í•œë‹¤ (ë°˜ëŒ€ì˜ ê²½ìš°, ì¦‰, íŠ¹ì • ë…¸ë“œë¡œ í–¥í•˜ëŠ” ì—£ì§€ê°€ ìžˆëŠ” ê²½ìš°ëŠ” *in-neighbors(incoming neighbors)*ë¼ í•œë‹¤).
ê·¸ëŸ¬ë©´ generalized attention mechanismì˜ $i^{\text{th}}$ output vectorëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

$$
\text{Attn} _D (\mathbf X) _i = \mathbf x _i + \sum^H _{h=1} \sigma (Q _h (\mathbf x _i) K _h (\mathbf X _{N(i)})^\intercal) \cdot V _h (\mathbf X _{N(i)}) \tag{AT}
$$

ì—¬ê¸°ì„œ $Q _h, K _h : \mathbb R^{d} \to \mathbb R^{m}, V _h: \mathbb R^{d} \to \mathbb R^{d}$ì€ ê° ê° query, key, value functionì„ ì˜ë¯¸í•˜ê³ , $\sigma$ëŠ” hardmaxë‚˜ softmaxê°™ì€ scoring functionì„ ì˜ë¯¸í•œë‹¤. $H$ëŠ” multi-head attentionì˜ ê°¯ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.
ë˜í•œ, $X _{N(i)}$ëŠ” ëª¨ë“  ì¸í’‹ì´ ì•„ë‹Œ $\mathbf x _j : j \in N(i)$ì— í•´ë‹¹í•˜ëŠ” ì¸í’‹ìœ¼ë¡œë§Œ êµ¬ì„±ëœ í–‰ë ¬ì„ ì˜ë¯¸í•œë‹¤.
ë§Œì•½ $D$ê°€ complete digraphë¼ë©´ ì´ëŠ” vanilla transformerê°€ ëœë‹¤.

ì§€ê¸ˆê¹Œì§€ì˜ notationì„ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ë©´, ê·¸ëž˜í”„ $D$ì˜ adjacency matrix $A$ì— ì ìš©ë˜ëŠ” ì—°ì‚°ì´ë©°, $A \in [0, 1]^{n \times n}$ì´ê³  ê° ì›ì†Œ $A(i, j)$ëŠ” query $i$ê°€ key $j$ì— attendí•˜ë©´ 1ì´ ë˜ê³  ê·¸ ì™¸ì˜ ê²½ìš°ì—” 0ì´ ëœë‹¤.
ë§ˆì°¬ê°€ì§€ë¡œ full attentionì˜ ê²½ìš° $A$ì˜ ëª¨ë“  ì›ì†ŒëŠ” 1ì´ ë˜ë©°, quadratic complexityë¥¼ ê°–ê²Œ ëœë‹¤.

ì´ëŸ¬í•œ self-attentionì„ fully connected graphë¡œ ë³´ëŠ” ê´€ì ì€ ê·¸ëž˜í”„ ì´ë¡ ì„ í™œìš©í•  ìˆ˜ ìžˆê²Œ í•´ì£¼ê³ , ê·¸ëž˜í”„ ê´€ë ¨ ë°©ë²•ë¡ ì„ ì´ìš©í•´ì„œ ë³µìž¡ë„ë¥¼ ì¤„ì¼ ìˆ˜ ìžˆê²Œëœë‹¤.
ë”°ë¼ì„œ self-attentionì˜ quadratic complexityëŠ” ì´ì œ *graph sparsification problem*ìœ¼ë¡œ ë°”ë€Œê²Œ ëœë‹¤.
ë˜í•œ, ìž„ì˜ì˜ ê·¸ëž˜í”„ëŠ” expanderì´ë©°, spectral property ë“±ì˜ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ complete graphë¡œ ê·¼ì‚¬í•  ìˆ˜ ìžˆë‹¤.

ë³¸ ì–´í…ì…˜ì„ sparse random graphë¡œ í‘œí˜„í•˜ëŠ” ê²½ìš° ë§Œì¡±í•´ì•¼ í•˜ëŠ” ê²ƒì€ ë‘ ê°€ì§€ì´ë‹¤. 
ì²«ë²ˆì§¸ëŠ” ë…¸ë“œë“¤ê°„ í‰ê·  ì´ë™ ê±°ë¦¬ê°€ ìž‘ì•„ì•¼ í•˜ê³ , ë‘ë²ˆì§¸ëŠ” localityì´ë‹¤.

### small average path length between node

*ErdÅ‘sâ€“RÃ©nyi model*ë¡œë„ ì•Œë ¤ì§„ ê°€ìž¥ ë‹¨ìˆœí•œ random graph constructionì„ ìƒê°í•´ë³´ìž.
ì´ ê²½ìš° ê° ì—£ì§€ëŠ” ê³ ì •ëœ í™•ë¥ ë¡œ ë…ë¦½ë˜ê²Œ ì„ íƒëœë‹¤.
$\tilde{\Theta} (n)$ê°œì˜ ì—£ì§€ë¥¼ ê°–ëŠ” ê·¸ëž˜í”„ì˜ ê²½ìš° ë‘ ë…¸ë“œ ê°„ì— ê°€ìž¥ ì§§ì€ ê²½ë¡œëŠ” ë…¸ë“œì˜ ê°¯ìˆ˜ì— logarithmicí•˜ë‹¤.
ê·¸ ê²°ê³¼ ìž„ì˜ì˜ ê·¸ëž˜í”„ëŠ” complete graphì— spectrally approximateí•˜ë©°, adjacency matrixì˜ ë‘ë²ˆì§¸ eigenvalueëŠ” ì²«ë²ˆì§¸ eigenvalueë¡œë¶€í„° ë©€ë¦¬ ë–¨ì–´ì§€ê²Œ ëœë‹¤.

ì´ëŸ¬í•œ ì„±ì§ˆì€ random walkì—ì„œì˜ *mixing time*ì„ ë¹ ë¥´ê²Œ ë§Œë“¤ì–´ì£¼ê³ , ì´ë¡œì¸í•´ ì–´ë– í•œ ë…¸ë“œ ì‚¬ì´ì—ë„ ì •ë³´ê°€ ë¹ ë¥´ê²Œ íë¥¼ ìˆ˜ ìžˆë„ë¡ í•œë‹¤.
ë”°ë¼ì„œ sparse attentionì„ ì œì•ˆ, ê° ì¿¼ë¦¬ê°€ ìž„ì˜ì˜ $r$ê°œì˜ í‚¤ì— attention í•˜ë„ë¡í•œë‹¤. 

> Mixing time is essentially the time it takes for the *chain* to reach or get close to the *stationary distribution*.
> 
> Definition 14.3 (Mixing Time). The mixing time of the chain corresponding to a graph $G$ is the smallest time $t$ such that for any starting distribution $x$,
> $$
> xP^t - \pi \leq 1/4
> $$

chain?
stationary distribution?


### Notion of locality

NLPì™€ computational biologyì—ì„  ëŒ€ë¶€ë¶„ì˜ ë¬¸ë§¥ì´ ìƒë‹¹ížˆ ë§Žì€ ì–‘ì˜ *locality of reference*ë¥¼ ë³´ì—¬ì£¼ëŠ” ë°ì´í„°ë¥¼ ê°–ëŠ”ë‹¤.
ì—¬ê¸°ì„œëŠ” ì–´ë–¤ í† í°ì— ëŒ€í•œ ë§Žì€ ì •ë³´ê°€ ì´ì˜ ì´ì›ƒí•œ í† í°ìœ¼ë¡œë¶€í„° ì–»ì–´ì§ˆ ìˆ˜ ìžˆë‹¤ëŠ” ì ì´ë‹¤.
Clark et al. [19]ëŠ” NLPì˜ self-attention modelì´ ì´ì›ƒí•œ í† í°ê³¼ì˜ inner-productê°€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤ê³  ê²°ë¡ ë‚´ì—ˆë‹¤.
Locality, ì¦‰, í† í°ì˜ ê·¼ì ‘ì„±ì€ transformational-generative grammarì™€ ê°™ì€ ë‹¤ì–‘í•œ ì–¸ì–´ì  ì´ë¡ ì„ í˜•ì„±í•œë‹¤.

transformational grammar: a system of language analysis that recognizes the relationship among the various elements of a sentence and among the possible sentences of a language and uses processes or rules (some of which are called transformations) to express these relationships.

ê·¸ëž˜í”„ ì´ë¡ ì—ì„œëŠ” clustering coefficientê°€ connectivityì— ëŒ€í•œ localityë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì´ê³ , ë§Žì€ cliquesë‚˜ near-cliques(subgraphs that
are almost fully interconnected)ë¥¼ í¬í•¨í• ìˆ˜ë¡ ë†’ì€ ê°’ì„ ê°–ëŠ”ë‹¤.

ë‹¨ìˆœí•œ ErdÅ‘sâ€“RÃ©nyi random graphsëŠ” ë†’ì€ clustering coefficientë¥¼ ê°–ì§„ ì•Šì§€ë§Œ small world graphë¡œ ì•Œë ¤ì§„ a class of random graphs ë†’ì€ clustering coefficientë¥¼ ê°–ëŠ”ë‹¤.
Watts and Strogatz [94]ì˜ ëª¨ë¸ì€ average shortest pathì™€ notion of locality ë‘˜ ì‚¬ì´ì—ì„œ ì¢‹ì€ ê· í˜•ì„ ì´ë£¨ê³  ìžˆê¸° ë•Œë¬¸ì— BigBirdì™€ ìœ ì‚¬ì„±ì´ ë†’ë‹¤.
ì´ë“¤ì˜ ëª¨ë¸ì€ regular ring latticeë¥¼ ìƒì„±í•œ í›„ $n$ê°œì˜ ë…¸ë“œê°€ ê° ë°©í–¥ìœ¼ë¡œ $w/2$ê°œì˜ ì´ì›ƒë“¤ê³¼ ì—°ê²°ë˜ëŠ” ì‹ìœ¼ë¡œ ìƒì„±ëœë‹¤.

ì´ëŸ¬í•œ í˜•íƒœì˜ attentionì„ **sliding window attention**ë¡œ ì •ì˜í•œë‹¤.
Sliding window attentionì—ì„  width $w$ë¥¼ ê°–ëŠ” self-attentionì´ $i$ë²ˆì§¸ queryê°€ $i-w/2$ë¶€í„° $i+w/2$ì˜ keyê¹Œì§€ attendí•œë‹¤.
ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
A(i, i-w/2:i+w/2)=1
$$

ê·¸ëŸ¬ë‚˜ ë³¸ ì•„ì´ë””ì–´ì— ëŒ€í•œ sanity checkë¡œ BERTì˜ ì„±ëŠ¥ì— ê·¼ì ‘í•  ìˆ˜ ìžˆëŠ”ì§€ í…ŒìŠ¤íŠ¸í•´ë³¸ ê²°ê³¼ random blocksê³¼ local windowë¡œëŠ” ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ëŠ” ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤.

![image](https://user-images.githubusercontent.com/47516855/190448873-8caf058d-fea1-45c4-a5b3-707eecda68e2.png){: .align-center}{: width="300"}

ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì˜ theoretical analysis (Sec. 3)ì— ê¸°ë°˜, **global tokens**ì„ ì¶”ê°€í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒ.
ì´ëŠ” ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìžˆëŠ”ë°,

- BigBird-ITC: ê¸°ì¡´ì— ì¡´ìž¬í•˜ë˜ í† í°ì„ **global**í•˜ê²Œ ë§Œë“¬ (internal transformer construction, ITC)
- BigBird-ETC: CLSê°™ì€ ìƒˆë¡œìš´ í† í°ì„ ë§Œë“¤ì–´ **global**í•˜ê²Œ ë§Œë“¬ (Extended Transformer Construction, ETC)

### Implementation details

GPUë‚˜ TPUê°™ì€ hardware acceleratorëŠ” ì—°ì†ëœ byte blockì„ í•œë²ˆì— loadí•˜ëŠ” coalesced memory operationì—ì„œ ìœ ìš©í•˜ë‹¤. 
ê·¸ëŸ¬ë‚˜ ë³¸ BigBirdì—ì„œ ë™ìž‘í•˜ëŠ” sliding windowë‚˜ random element queryë¡œ ì¸í•œ ì†ŒëŸ‰ì˜ ë¹„ê·œì¹™ì ì¸ look-upì˜ ê²½ìš° ì´ëŸ¬í•œ acceleratorë¥¼ í™œìš©í•˜ê¸°ê°€ ì–´ë µë‹¤.
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” look-upì„ **blockifying**(ë¸”ë¡í™”)í•˜ì—¬ í•´ê²°í•œë‹¤.

ì•žì„œ ë³¸ adjacency matrixê°€ sparseí•œ ê²½ìš°ì—” ì´ëŠ” GPUì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„í•˜ê¸°ê°€ ì–´ë µë‹¤.
GPUëŠ” ëª‡ì²œê°œì˜ ì½”ì–´ë¥¼ í†µí•´ ì—°ì‚°ì„ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

ì´ë¥¼ ìœ„í•˜ì—¬ attention patternì„ ë¸”ë¡í™”í•œë‹¤.
ì¦‰, queryì™€ keyë¥¼ í•¨ê»˜ packingí•œ í›„, ì´ ë¸”ë¡ì— ëŒ€í•´ attentionì„ ì •ì˜í•œë‹¤.

ê·¸ë¦¼ê³¼ ê°™ì´ 12ê°œì˜ query/key vectorê°€ ìžˆê³ , block sizeë¥¼ 2ë¼ í•˜ìž.
ê·¸ëŸ¬ë©´ query/key matrixë¥¼ $12/2=6$ê°œì˜ blockìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìžˆë‹¤.
ì´ë ‡ê²Œ ì™„ì„±ëœ block matrixì— ëŒ€í•´ ì•žì„œ ì‚´íŽ´ë³¸ attention patternì„ ì ìš©í•œë‹¤.

1. Random attention: ê° query blockì´ $r$ê°œì˜ ìž„ì˜ì˜ key blockìœ¼ë¡œ attendí•œë‹¤. ê·¸ë¦¼ì˜ ê²½ìš° $r=1$ì´ ëœë‹¤.
2. Window local attention: query/key blockì˜ ìˆ˜ê°€ ê°™ì•„ì•¼ block windowë¥¼ ì§„í–‰í•  ìˆ˜ ìžˆë‹¤. ëª¨ë“  $j$ë²ˆì§¸ query blockì´ $j-(w-1)/2$ë¶€í„° $j+(w-1)/2$ blockê¹Œì§€ attendí•œë‹¤. ê·¸ë¦¼ì˜ ê²½ìš° $w=3$ì´ ëœë‹¤.

Turing Complete(íŠœë§ ì™„ì „)
íŠœë§ ì™„ì „(turing complete)ì´ëž€ ì–´ë–¤ í”„ë¡œê·¸ëž˜ë° ì–¸ì–´ë‚˜ ì¶”ìƒ ë¨¸ì‹ ì´ íŠœë§ ë¨¸ì‹ ê³¼ ë™ì¼í•œ ê³„ì‚° ëŠ¥ë ¥ì„ ê°€ì§„ë‹¤ëŠ” ì˜ë¯¸ì´ë©° íŠœë§ ë¨¸ì‹ ìœ¼ë¡œ í’€ ìˆ˜ ìžˆëŠ” ë¬¸ì œ, ì¦‰ ê³„ì‚°ì ì¸ ë¬¸ì œë¥¼ ê·¸ í”„ë¡œê·¸ëž˜ë° ì–¸ì–´ë‚˜ ì¶”ìƒ ë¨¸ì‹ ìœ¼ë¡œ í’€ ìˆ˜ ìžˆë‹¤ëŠ” ì˜ë¯¸.

> The point of stating that a mathematical model is Turing Complete is to reveal the capability of the model to perform any calculation, given a sufficient amount of resources (i.e. infinite), not to show whether a specific implementation of a model does have those resources. Non-Turing complete models would not be able to handle a specific set of calculations, even with enough resources, something that reveals a difference in the way the two models operate, even when they have limited resources. Of course, to prove this property, you have to do have to assume that the models are able to use an infinite amount of resources, but this property of a model is relevant even when resources are limited. [ì¶œì²˜](https://stackoverflow.com/questions/2990277/how-useful-is-turing-completeness-are-neural-nets-turing-complete)

PÃ©rez et al. [72] showed that the full transformer based on a quadratic attention
mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the
model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers
are bounded finite state machines and cannot be Turing Complete. -> ????




## Summary

{: .align-center}{: width="300"}
